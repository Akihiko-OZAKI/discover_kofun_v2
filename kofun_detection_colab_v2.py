#!/usr/bin/env python3
"""
å¤å¢³æ¤œå‡ºãƒ¢ãƒ‡ãƒ«å­¦ç¿’ - Google Colab V2æœ€é©åŒ–ç‰ˆ
å›½åœŸåœ°ç†é™¢ã®DEM5Aãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å¤å¢³æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import requests
import zipfile
import json
import pandas as pd
import numpy as np
from pathlib import Path
import time
import concurrent.futures
from tqdm import tqdm
import xml.etree.ElementTree as ET
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import rasterio
from rasterio.transform import from_bounds

# å¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
kofun_coordinates = [
    [1, 34.698046, 135.809032],  # ä»å¾³å¤©çš‡é™µ
    [2, 34.697919, 135.804807],  # å±¥ä¸­å¤©çš‡é™µ
    [3, 34.701343, 135.802867],  # åæ­£å¤©çš‡é™µ
    [4, 34.694167, 135.792778],  # å¿œç¥å¤©çš‡é™µ
    [5, 34.691667, 135.790278],  # ä»²å“€å¤©çš‡é™µ
]

class GSIDataCollectorV2:
    def __init__(self, output_dir="collected_data_v2"):
        self.base_url = "https://fgd.gsi.go.jp/download/API2/contents/XML/"
        self.output_dir = output_dir
        self.dem_dir = os.path.join(output_dir, "dem5a")
        self.processed_dir = os.path.join(output_dir, "processed")
        
        os.makedirs(self.dem_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def get_dem5a_list(self, lat_min, lat_max, lon_min, lon_max):
        """æŒ‡å®šç¯„å›²ã®DEM5Aãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        tile_list_url = "https://fgd.gsi.go.jp/download/API2/contents/XML/dem5a.xml"
        
        try:
            response = requests.get(tile_list_url)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            tiles = []
            
            for tile in root.findall(".//tile"):
                lat = float(tile.find("lat").text)
                lon = float(tile.find("lon").text)
                
                if lat_min <= lat <= lat_max and lon_min <= lon <= lon_max:
                    tile_info = {
                        'lat': lat,
                        'lon': lon,
                        'filename': tile.find("filename").text,
                        'url': tile.find("url").text
                    }
                    tiles.append(tile_info)
            
            return tiles
            
        except Exception as e:
            print(f"ã‚¿ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def download_dem5a(self, tile_info):
        """å€‹åˆ¥ã®DEM5Aãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        filename = tile_info['filename']
        url = tile_info['url']
        output_path = os.path.join(self.dem_dir, filename)
        
        if os.path.exists(output_path):
            return True
        
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {filename} - {e}")
            return False
    
    def download_range(self, lat_min, lat_max, lon_min, lon_max, delay=1):
        """æŒ‡å®šç¯„å›²ã®DEM5Aãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        print(f"ğŸŒ ç¯„å›²: ç·¯åº¦ {lat_min}-{lat_max}, çµŒåº¦ {lon_min}-{lon_max}")
        
        tiles = self.get_dem5a_list(lat_min, lat_max, lon_min, lon_max)
        print(f"ğŸ“Š å¯¾è±¡ã‚¿ã‚¤ãƒ«æ•°: {len(tiles)}")
        
        success_count = 0
        for i, tile in enumerate(tqdm(tiles, desc="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­")):
            if self.download_dem5a(tile):
                success_count += 1
            
            time.sleep(delay)
        
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {success_count}/{len(tiles)} ãƒ•ã‚¡ã‚¤ãƒ«")
        return success_count

class DataPreprocessorV2:
    def __init__(self, kofun_coordinates):
        self.kofun_coordinates = kofun_coordinates
        
    def xml_to_image(self, xml_path, output_path):
        """XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”»åƒã«å¤‰æ›"""
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()
            
            bounds = root.find('.//{http://www.opengis.net/gml}Envelope')
            if bounds is None:
                return False
                
            lower_corner = bounds.find('.//{http://www.opengis.net/gml}lowerCorner').text.split()
            upper_corner = bounds.find('.//{http://www.opengis.net/gml}upperCorner').text.split()
            
            lon_min, lat_min = map(float, lower_corner)
            lon_max, lat_max = map(float, upper_corner)
            
            grid_data = root.find('.//{http://www.opengis.net/gml}tupleList')
            if grid_data is None:
                return False
                
            lines = grid_data.text.strip().split('\n')
            elevation_data = []
            
            for line in lines:
                if line.strip():
                    parts = line.split(',')
                    if len(parts) >= 3:
                        elevation = float(parts[2])
                        elevation_data.append(elevation)
            
            size = int(len(elevation_data) ** 0.5)
            if size * size != len(elevation_data):
                size = int(len(elevation_data) ** 0.5) + 1
            
            grid = np.array(elevation_data[:size*size]).reshape(size, size)
            grid_normalized = ((grid - grid.min()) / (grid.max() - grid.min()) * 255).astype(np.uint8)
            
            cv2.imwrite(output_path, grid_normalized)
            return True
            
        except Exception as e:
            print(f"XMLå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_labels(self, xml_path, image_path, label_path):
        """å¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ"""
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()
            
            bounds = root.find('.//{http://www.opengis.net/gml}Envelope')
            lower_corner = bounds.find('.//{http://www.opengis.net/gml}lowerCorner').text.split()
            upper_corner = bounds.find('.//{http://www.opengis.net/gml}upperCorner').text.split()
            
            lon_min, lat_min = map(float, lower_corner)
            lon_max, lat_max = map(float, upper_corner)
            
            img = cv2.imread(image_path)
            img_height, img_width = img.shape[:2]
            
            labels = []
            for kofun in self.kofun_coordinates:
                kofun_lat, kofun_lon = kofun[1], kofun[2]
                
                if lat_min <= kofun_lat <= lat_max and lon_min <= kofun_lon <= lon_max:
                    x = (kofun_lon - lon_min) / (lon_max - lon_min)
                    y = 1 - (kofun_lat - lat_min) / (lat_max - lat_min)
                    
                    x_center = x
                    y_center = y
                    width = 0.05
                    height = 0.05
                    
                    labels.append(f"0 {x_center} {y_center} {width} {height}")
            
            with open(label_path, 'w') as f:
                f.write('\n'.join(labels))
            
            return len(labels)
            
        except Exception as e:
            print(f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return 0

def create_training_dataset_v2(collected_data_dir, kofun_coordinates):
    """V2æœ€é©åŒ–ç‰ˆã®å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    print("ğŸ“Š V2æœ€é©åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
    
    dataset_dir = "dataset_v2"
    os.makedirs(f"{dataset_dir}/images/train", exist_ok=True)
    os.makedirs(f"{dataset_dir}/images/val", exist_ok=True)
    os.makedirs(f"{dataset_dir}/labels/train", exist_ok=True)
    os.makedirs(f"{dataset_dir}/labels/val", exist_ok=True)
    
    preprocessor = DataPreprocessorV2(kofun_coordinates)
    
    xml_files = list(Path(collected_data_dir).glob("**/*.xml"))
    print(f"ğŸ“ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(xml_files)}")
    
    train_count = 0
    val_count = 0
    
    for i, xml_file in enumerate(tqdm(xml_files, desc="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­")):
        try:
            image_path = f"{dataset_dir}/images/temp_{i}.png"
            if preprocessor.xml_to_image(str(xml_file), image_path):
                
                label_path = f"{dataset_dir}/labels/temp_{i}.txt"
                label_count = preprocessor.generate_labels(str(xml_file), image_path, label_path)
                
                if label_count > 0:
                    if np.random.random() < 0.8:
                        final_image_path = f"{dataset_dir}/images/train/train_{train_count:04d}.png"
                        final_label_path = f"{dataset_dir}/labels/train/train_{train_count:04d}.txt"
                        train_count += 1
                    else:
                        final_image_path = f"{dataset_dir}/images/val/val_{val_count:04d}.png"
                        final_label_path = f"{dataset_dir}/labels/val/val_{val_count:04d}.txt"
                        val_count += 1
                    
                    os.rename(image_path, final_image_path)
                    os.rename(label_path, final_label_path)
                else:
                    os.remove(image_path)
                    if os.path.exists(label_path):
                        os.remove(label_path)
            
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {xml_file}: {e}")
            continue
    
    dataset_info = {
        'train_count': train_count,
        'val_count': val_count,
        'total_count': train_count + val_count
    }
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: è¨“ç·´ {train_count}ä»¶, æ¤œè¨¼ {val_count}ä»¶")
    return dataset_info

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ å¤å¢³æ¤œå‡ºãƒ¢ãƒ‡ãƒ«å­¦ç¿’ - Google Colab V2æœ€é©åŒ–ç‰ˆ")
    print("=" * 60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿åé›†
    print("\nğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—1: å›½åœŸåœ°ç†é™¢ãƒ‡ãƒ¼ã‚¿åé›†")
    collector = GSIDataCollectorV2()
    
    areas = [
        (34.4, 35.0, 135.4, 136.0),  # å¤§é˜ªãƒ»å¥ˆè‰¯å‘¨è¾º
        (34.8, 35.2, 135.6, 135.9),  # äº¬éƒ½å‘¨è¾º
        (34.6, 34.8, 135.7, 135.9),  # å ºå¸‚å‘¨è¾º
    ]
    
    total_collected = 0
    for i, area in enumerate(areas, 1):
        print(f"\nğŸ“¦ åœ°åŸŸ {i}: {area}")
        collected = collector.download_range(*area, delay=0.5)
        total_collected += collected
    
    print(f"\nğŸ‰ ç·åé›†ãƒ‡ãƒ¼ã‚¿æ•°: {total_collected}ä»¶")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    dataset_info = create_training_dataset_v2("collected_data_v2/dem5a", kofun_coordinates)
    
    # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    print("\nâš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
    dataset_config_v2 = f"""
# Kofun Detection Dataset V2 Configuration
path: ./dataset_v2  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')

# Classes
nc: 1  # number of classes
names: ['kofun']  # class names

# V2 Optimization Settings
optimization:
  ensemble_enabled: true
  augmentation_enabled: true
  validation_enhanced: true
"""
    
    with open('kofun_dataset_v2.yaml', 'w') as f:
        f.write(dataset_config_v2)
    
    print("âœ… V2æœ€é©åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±: {dataset_info}")
    
    # 4. å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆ
    print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("=" * 50)
    print("cd yolov5")
    print("python train.py --img 640 --batch 16 --epochs 100 --data ../kofun_dataset_v2.yaml --weights yolov5s.pt --cache --patience 20 --save-period 10 --project runs/train --name kofun_v2_optimized --exist-ok")
    print("=" * 50)
    
    print("\nğŸ‰ V2æœ€é©åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å­¦ç¿’æº–å‚™å®Œäº†ï¼")
    print("\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•")
    print("2. ä¸Šè¨˜ã®å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ")
    print("3. å­¦ç¿’å®Œäº†å¾Œã€best.ptã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print("4. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®yolov5/weights/ã«é…ç½®")

if __name__ == "__main__":
    main() 