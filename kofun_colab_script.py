#!/usr/bin/env python3
"""
å¤å¢³æ¤œå‡ºãƒ¢ãƒ‡ãƒ«å­¦ç¿’ - Google Colabç‰ˆ
æ›´æ–°ã•ã‚ŒãŸå¤å¢³ãƒªã‚¹ãƒˆï¼ˆ63ä»¶ï¼‰ã‚’ä½¿ç”¨
"""

import os
import requests
import zipfile
import json
import pandas as pd
import numpy as np
from pathlib import Path
import time
import concurrent.futures
from tqdm import tqdm
import xml.etree.ElementTree as ET
import cv2
from PIL import Image
import matplotlib.pyplot as plt

# æ›´æ–°ã•ã‚ŒãŸå¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆ63ä»¶ï¼‰
def load_kofun_coordinates():
    """æ—¢çŸ¥ã®å¤å¢³ãƒªã‚¹ãƒˆã‹ã‚‰åº§æ¨™ã‚’èª­ã¿è¾¼ã¿"""
    # æ›´æ–°ã•ã‚ŒãŸå¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆ63ä»¶ï¼‰
    kofun_coordinates = [
        # æ—¢å­˜ã®å¤å¢³ï¼ˆ54ä»¶ï¼‰
        [1, 34.698046, 135.809032],  # ä»å¾³å¤©çš‡é™µ
        [2, 34.697919, 135.804807],  # å±¥ä¸­å¤©çš‡é™µ
        [3, 34.701343, 135.802867],  # åæ­£å¤©çš‡é™µ
        [4, 34.696442, 135.797175],  # å¿œç¥å¤©çš‡é™µ
        [5, 34.700241, 135.791136],  # ä»²å“€å¤©çš‡é™µ
        [6, 34.698339, 135.78764],   # å¤å¢³6
        [7, 34.678475, 135.781787],  # å¤å¢³7
        [8, 34.683137, 135.76917],   # å¤å¢³8
        [9, 34.706142, 135.78765],   # å¤å¢³9
        [10, 34.703795, 135.793919], # å¤å¢³10
        [11, 34.57611111, 135.4883], # å¤å¢³11
        [12, 34.568056, 135.486667], # å¤å¢³12
        [13, 34.565, 135.491111],    # å¤å¢³13
        [14, 34.562778, 135.490556], # å¤å¢³14
        [15, 34.558611, 135.487778], # å¤å¢³15
        [16, 34.56, 135.485],        # å¤å¢³16
        [17, 34.561111, 135.483333], # å¤å¢³17
        [18, 34.562778, 135.482222], # å¤å¢³18
        [19, 34.566944, 135.484167], # å¤å¢³19
        [20, 34.566944, 135.485278], # å¤å¢³20
        [21, 34.558056, 135.487778], # å¤å¢³21
        [22, 34.556667, 135.482778], # å¤å¢³22
        [23, 34.555, 135.484167],    # å¤å¢³23
        [24, 34.553889, 135.4775],   # å¤å¢³24
        [25, 34.556111, 135.48],     # å¤å¢³25
        [26, 34.556667, 135.479444], # å¤å¢³26
        [27, 34.553056, 135.485833], # å¤å¢³27
        [28, 34.5525, 135.486389],   # å¤å¢³28
        [29, 34.554722, 135.490833], # å¤å¢³29
        [30, 34.546667, 135.499444], # å¤å¢³30
        [31, 34.581944, 135.593611], # å¤å¢³31
        [32, 34.565833, 135.594167], # å¤å¢³32
        [33, 34.567778, 135.595833], # å¤å¢³33
        [34, 34.573056, 135.616667], # å¤å¢³34
        [35, 34.581944, 135.593611], # å¤å¢³35
        [36, 34.571389, 135.581389], # å¤å¢³36
        [37, 34.568056, 135.613056], # å¤å¢³37
        [38, 34.568056, 135.613611], # å¤å¢³38
        [39, 34.568056, 135.614444], # å¤å¢³39
        [40, 34.568056, 135.609444], # å¤å¢³40
        [41, 34.566944, 135.608889], # å¤å¢³41
        [42, 34.562222, 135.609444], # å¤å¢³42
        [43, 34.563889, 135.612222], # å¤å¢³43
        [44, 34.562778, 135.6125],   # å¤å¢³44
        [45, 34.561667, 135.605278], # å¤å¢³45
        [46, 34.561667, 135.602222], # å¤å¢³46
        [47, 34.557778, 135.604444], # å¤å¢³47
        [48, 34.558889, 135.604444], # å¤å¢³48
        [49, 34.557222, 135.606111], # å¤å¢³49
        [50, 34.556111, 135.606667], # å¤å¢³50
        [51, 34.556944, 135.601944], # å¤å¢³51
        [52, 34.556111, 135.600556], # å¤å¢³52
        [53, 34.5525, 135.5975],     # å¤å¢³53
        [54, 34.551111, 135.604444], # å¤å¢³54
        # ã•ããŸã¾å²è·¡ã®å¤å¢³ï¼ˆ9åŸºï¼‰
        [55, 36.1263, 139.5575],     # ã•ããŸã¾å²è·¡åšç‰©é¤¨å‘¨è¾º
        [56, 36.1250, 139.5580],     # å¤å¢³1
        [57, 36.1270, 139.5565],     # å¤å¢³2
        [58, 36.1245, 139.5590],     # å¤å¢³3
        [59, 36.1280, 139.5560],     # å¤å¢³4
        [60, 36.1235, 139.5595],     # å¤å¢³5
        [61, 36.1290, 139.5555],     # å¤å¢³6
        [62, 36.1225, 139.5600],     # å¤å¢³7
        [63, 36.1300, 139.5550],     # å¤å¢³8
    ]
    
    print(f"âœ… å¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {len(kofun_coordinates)}ä»¶")
    return kofun_coordinates

class GSIDataCollector:
    def __init__(self, output_dir="collected_data"):
        self.base_url = "https://fgd.gsi.go.jp/download/API2/contents/XML/"
        self.output_dir = output_dir
        self.dem_dir = os.path.join(output_dir, "dem5a")
        
        os.makedirs(self.dem_dir, exist_ok=True)
        
    def get_dem5a_list(self, lat_min, lat_max, lon_min, lon_max):
        """æŒ‡å®šç¯„å›²ã®DEM5Aãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        tile_list_url = "https://fgd.gsi.go.jp/download/API2/contents/XML/dem5a.xml"
        
        try:
            response = requests.get(tile_list_url)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            tiles = []
            
            for tile in root.findall(".//tile"):
                lat = float(tile.find("lat").text)
                lon = float(tile.find("lon").text)
                
                if lat_min <= lat <= lat_max and lon_min <= lon <= lon_max:
                    tile_info = {
                        'lat': lat,
                        'lon': lon,
                        'filename': tile.find("filename").text,
                        'url': tile.find("url").text
                    }
                    tiles.append(tile_info)
            
            return tiles
            
        except Exception as e:
            print(f"ã‚¿ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def download_dem5a(self, tile_info):
        """å€‹åˆ¥ã®DEM5Aãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        filename = tile_info['filename']
        url = tile_info['url']
        output_path = os.path.join(self.dem_dir, filename)
        
        if os.path.exists(output_path):
            return True
        
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {filename} - {e}")
            return False
    
    def download_range(self, lat_min, lat_max, lon_min, lon_max, delay=1):
        """æŒ‡å®šç¯„å›²ã®DEM5Aãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        print(f"ğŸŒ ç¯„å›²: ç·¯åº¦ {lat_min}-{lat_max}, çµŒåº¦ {lon_min}-{lon_max}")
        
        tiles = self.get_dem5a_list(lat_min, lat_max, lon_min, lon_max)
        
        if not tiles:
            print("âš ï¸ è©²å½“ã™ã‚‹ã‚¿ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return 0
        
        print(f"ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡: {len(tiles)}ä»¶")
        
        success_count = 0
        for tile in tqdm(tiles, desc="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­"):
            if self.download_dem5a(tile):
                success_count += 1
            time.sleep(delay)
        
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {success_count}/{len(tiles)}ä»¶")
        return success_count

class DataPreprocessor:
    def __init__(self, kofun_coordinates):
        self.kofun_coordinates = kofun_coordinates
        
    def xml_to_image(self, xml_path, output_path):
        """XMLã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ"""
        try:
            # XMLã‚’ç›´æ¥è§£æ
            tree = ET.parse(xml_path)
            root = tree.getroot()
            
            # åº§æ¨™ç¯„å›²ã‚’å–å¾—
            envelope = root.find('.//{http://www.opengis.net/gml/3.2}Envelope')
            if envelope is not None:
                lower_corner = envelope.find('.//{http://www.opengis.net/gml/3.2}lowerCorner').text
                upper_corner = envelope.find('.//{http://www.opengis.net/gml/3.2}upperCorner').text
                
                # ã‚°ãƒªãƒƒãƒ‰æƒ…å ±ã‚’å–å¾—
                grid = root.find('.//{http://www.opengis.net/gml/3.2}Grid')
                if grid is not None:
                    limits = grid.find('.//{http://www.opengis.net/gml/3.2}GridEnvelope')
                    low = limits.find('.//{http://www.opengis.net/gml/3.2}low').text
                    high = limits.find('.//{http://www.opengis.net/gml/3.2}high').text
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    tuple_list = root.find('.//{http://www.opengis.net/gml/3.2}tupleList')
                    if tuple_list is not None:
                        # ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã¦ç”»åƒã‚’ç”Ÿæˆ
                        data_text = tuple_list.text.strip()
                        lines = data_text.split('\n')
                        
                        # ç°¡æ˜“çš„ãªç”»åƒç”Ÿæˆ
                        height = len(lines)
                        width = 225  # å›ºå®šå¹…
                        
                        image = np.zeros((height, width, 3), dtype=np.uint8)
                        
                        for i, line in enumerate(lines):
                            if i < height:
                                # ç°¡æ˜“çš„ãªé«˜åº¦ãƒãƒƒãƒ”ãƒ³ã‚°
                                try:
                                    elevation = float(line.split(',')[1])
                                    # é«˜åº¦ã‚’0-255ã«æ­£è¦åŒ–
                                    pixel_value = int((elevation - 0) / 100 * 255)
                                    pixel_value = max(0, min(255, pixel_value))
                                    
                                    image[i, :] = [pixel_value, pixel_value, pixel_value]
                                except:
                                    image[i, :] = [128, 128, 128]
                        
                        cv2.imwrite(output_path, image)
                        return True
            
            return False
                
        except Exception as e:
            print(f"âŒ ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_labels(self, xml_path, image_path, label_path):
        """å¤å¢³åº§æ¨™ã‹ã‚‰ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        try:
            # XMLã®åº§æ¨™ç¯„å›²ã‚’å–å¾—
            tree = ET.parse(xml_path)
            root = tree.getroot()
            
            envelope = root.find('.//{http://www.opengis.net/gml/3.2}Envelope')
            if envelope is None:
                return 0
            
            lower_corner = envelope.find('.//{http://www.opengis.net/gml/3.2}lowerCorner').text
            upper_corner = envelope.find('.//{http://www.opengis.net/gml/3.2}upperCorner').text
            
            lat_min, lon_min = map(float, lower_corner.split())
            lat_max, lon_max = map(float, upper_corner.split())
            
            # ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—
            image = cv2.imread(image_path)
            if image is None:
                return 0
            
            img_height, img_width = image.shape[:2]
            
            # å¤å¢³åº§æ¨™ã‚’ç”»åƒåº§æ¨™ã«å¤‰æ›
            labels = []
            for kofun_id, lat, lon in self.kofun_coordinates:
                if lat_min <= lat <= lat_max and lon_min <= lon <= lon_max:
                    # åº§æ¨™ã‚’ç”»åƒåº§æ¨™ã«å¤‰æ›
                    x_norm = (lon - lon_min) / (lon_max - lon_min)
                    y_norm = (lat - lat_min) / (lat_max - lat_min)
                    
                    # YOLOå½¢å¼ï¼ˆä¸­å¿ƒåº§æ¨™ã€å¹…ã€é«˜ã•ï¼‰
                    x_center = x_norm
                    y_center = 1.0 - y_norm  # Yè»¸ã‚’åè»¢
                    width = 0.05  # å›ºå®šã‚µã‚¤ã‚º
                    height = 0.05
                    
                    labels.append(f"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")
            
            # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            if labels:
                with open(label_path, 'w') as f:
                    f.write('\n'.join(labels))
                return len(labels)
            
            return 0
            
        except Exception as e:
            print(f"âŒ ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return 0

def create_training_dataset(collected_data_dir, kofun_coordinates):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹: {len(kofun_coordinates)}ä»¶ã®å¤å¢³åº§æ¨™")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    dataset_dir = "dataset_updated"
    os.makedirs(f"{dataset_dir}/images/train", exist_ok=True)
    os.makedirs(f"{dataset_dir}/images/val", exist_ok=True)
    os.makedirs(f"{dataset_dir}/labels/train", exist_ok=True)
    os.makedirs(f"{dataset_dir}/labels/val", exist_ok=True)
    
    preprocessor = DataPreprocessor(kofun_coordinates)
    
    # XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    xml_files = [f for f in os.listdir(collected_data_dir) if f.endswith('.xml')]
    
    train_count = 0
    val_count = 0
    
    for xml_file in tqdm(xml_files, desc="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­"):
        try:
            xml_path = os.path.join(collected_data_dir, xml_file)
            image_path = os.path.join(collected_data_dir, f"{xml_file[:-4]}.png")
            label_path = os.path.join(collected_data_dir, f"{xml_file[:-4]}.txt")
            
            # XML â†’ PNG å¤‰æ›
            if not preprocessor.xml_to_image(xml_path, image_path):
                continue
            
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
            label_count = preprocessor.generate_labels(xml_path, image_path, label_path)
            
            if label_count > 0:
                if np.random.random() < 0.8:
                    final_image_path = f"{dataset_dir}/images/train/train_{train_count:04d}.png"
                    final_label_path = f"{dataset_dir}/labels/train/train_{train_count:04d}.txt"
                    train_count += 1
                else:
                    final_image_path = f"{dataset_dir}/images/val/val_{val_count:04d}.png"
                    final_label_path = f"{dataset_dir}/labels/val/val_{val_count:04d}.txt"
                    val_count += 1
                
                os.rename(image_path, final_image_path)
                os.rename(label_path, final_label_path)
            else:
                os.remove(image_path)
                if os.path.exists(label_path):
                    os.remove(label_path)
        
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {xml_file}: {e}")
            continue
    
    dataset_info = {
        'train_count': train_count,
        'val_count': val_count,
        'total_count': train_count + val_count,
        'kofun_coordinates_count': len(kofun_coordinates)
    }
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: è¨“ç·´ {train_count}ä»¶, æ¤œè¨¼ {val_count}ä»¶")
    return dataset_info

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ å¤å¢³æ¤œå‡ºãƒ¢ãƒ‡ãƒ«å­¦ç¿’ - Google Colabç‰ˆï¼ˆæ›´æ–°ç‰ˆï¼‰")
    print("=" * 60)
    
    # å¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—0: å¤å¢³åº§æ¨™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    kofun_coordinates = load_kofun_coordinates()
    
    # åº§æ¨™ç¯„å›²ã‚’ç¢ºèª
    lats = [coord[1] for coord in kofun_coordinates]
    lons = [coord[2] for coord in kofun_coordinates]
    
    print(f"ğŸ“ åº§æ¨™ç¯„å›²:")
    print(f"   ç·¯åº¦: {min(lats):.6f} - {max(lats):.6f}")
    print(f"   çµŒåº¦: {min(lons):.6f} - {max(lons):.6f}")
    
    # 1. ãƒ‡ãƒ¼ã‚¿åé›†
    print("\nğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—1: å›½åœŸåœ°ç†é™¢ãƒ‡ãƒ¼ã‚¿åé›†")
    collector = GSIDataCollector()
    
    # æ›´æ–°ã•ã‚ŒãŸåœ°åŸŸãƒªã‚¹ãƒˆï¼ˆã•ããŸã¾å²è·¡ã‚’å«ã‚€ï¼‰
    areas = [
        (34.4, 35.0, 135.4, 136.0),  # å¤§é˜ªãƒ»å¥ˆè‰¯å‘¨è¾º
        (34.8, 35.2, 135.6, 135.9),  # äº¬éƒ½å‘¨è¾º
        (34.6, 34.8, 135.7, 135.9),  # å ºå¸‚å‘¨è¾º
        (36.1, 36.2, 139.4, 139.6),  # ã•ããŸã¾å²è·¡å‘¨è¾ºï¼ˆæ–°è¦è¿½åŠ ï¼‰
    ]
    
    total_collected = 0
    for i, area in enumerate(areas, 1):
        print(f"\nğŸ“¦ åœ°åŸŸ {i}: {area}")
        collected = collector.download_range(*area, delay=0.5)
        total_collected += collected
    
    print(f"\nğŸ‰ ç·åé›†ãƒ‡ãƒ¼ã‚¿æ•°: {total_collected}ä»¶")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    dataset_info = create_training_dataset("collected_data/dem5a", kofun_coordinates)
    
    # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    print("\nâš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
    dataset_config = f"""
# Kofun Detection Dataset Updated Configuration
path: ./dataset_updated  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')

# Classes
nc: 1  # number of classes
names: ['kofun']  # class names

# Updated Settings
optimization:
  ensemble_enabled: true
  augmentation_enabled: true
  validation_enhanced: true
  sakitama_included: true  # ã•ããŸã¾å²è·¡ã‚’å«ã‚€
"""
    
    with open('kofun_dataset_updated.yaml', 'w') as f:
        f.write(dataset_config)
    
    print("âœ… æ›´æ–°ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±: {dataset_info}")
    
    # 4. å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆ
    print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("=" * 50)
    print("cd yolov5")
    print("python train.py --img 640 --batch 16 --epochs 150 --data ../kofun_dataset_updated.yaml --weights yolov5s.pt --cache --patience 30 --save-period 10 --project runs/train --name kofun_updated --exist-ok")
    print("=" * 50)
    
    print("\nğŸ‰ æ›´æ–°ç‰ˆãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å­¦ç¿’æº–å‚™å®Œäº†ï¼")
    print("\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•")
    print("2. ä¸Šè¨˜ã®å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ")
    print("3. å­¦ç¿’å®Œäº†å¾Œã€best.ptã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print("4. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®yolov5/weights/ã«é…ç½®")
    print("5. ã•ããŸã¾å²è·¡ã§å†ãƒ†ã‚¹ãƒˆ")

if __name__ == "__main__":
    main() 